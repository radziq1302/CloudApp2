{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "import glob \n",
    "import string\n",
    "import re\n",
    "# import morfeusz2\n",
    "import nltk\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import FlairEmbeddings, BertEmbeddings, FastTextEmbeddings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tomek/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Supervised fastText models are not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-00d6dbdb65cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mextract_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings_of_entity_in_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0msave_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-00d6dbdb65cb>\u001b[0m in \u001b[0;36mget_embeddings_of_entity_in_corpus\u001b[0;34m(documents, window_size)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# Polish word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fasttext'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdocument_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-00d6dbdb65cb>\u001b[0m in \u001b[0;36mcreateEmbeddings\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFlairEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'polish-forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fasttext'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFastTextEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cc.pl.300.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_embeddings_of_entity_in_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/flair/embeddings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embeddings, use_local, field)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         self.precomputed_word_embeddings = gensim.models.FastText.load_fasttext_format(\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         )\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                 )\n\u001b[0;32m-> 1447\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mload_fasttext_format\u001b[0;34m(cls, model_file, encoding)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \"\"\"\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_facebook_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     @deprecated(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mload_facebook_model\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \"\"\"\n\u001b[0;32m-> 1250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_load_fasttext_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m_load_fasttext_format\u001b[0;34m(model_file, encoding, full_model)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \"\"\"\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fasttext_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m     model = FastText(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fin, encoding, full_model)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mraw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/_fasttext_bin.py\u001b[0m in \u001b[0;36m_load_vocab\u001b[0;34m(fin, new_format, encoding)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# Vocab stored by [Dictionary::save](https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnlabels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Supervised fastText models are not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading %s words for fastText model from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Supervised fastText models are not supported"
     ]
    }
   ],
   "source": [
    "PATH = 'categorization/learningData'\n",
    "\n",
    "\n",
    "with open('stopwords.txt', encoding='utf-8') as f:\n",
    "    STOPWORDS = f.read().splitlines()\n",
    "\n",
    "def load_files(path):\n",
    "    docs = []\n",
    "    files = glob.glob(path+'/**/*', recursive=True)\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(file, encoding='utf-8') as f:\n",
    "                docs.extend(f.read().splitlines())\n",
    "        except:\n",
    "            pass\n",
    "    return docs\n",
    "\n",
    "def remove_entity(sentence):\n",
    "    return re.sub('\\<(.*?)\\>','', sentence) \n",
    "\n",
    "def extract_lemm(morf):\n",
    "    return morf[0][2][1].split(':', 1)[0]\n",
    "\n",
    "def list_people(docs):\n",
    "    ## List all people appearing in documents \n",
    "    all_people = []\n",
    "    added_people = []\n",
    "    for doc in docs:\n",
    "        entities = re.findall('\\<Entity(.*?)\\>', doc)\n",
    "        for item in entities:\n",
    "            name = re.findall('name=\"(.*?)\"', item)[0]\n",
    "            if name not in added_people:\n",
    "                added_people.append(name)\n",
    "                try:\n",
    "                    person = {\n",
    "                        'name': re.findall('name=\"(.*?)\"', item)[0],\n",
    "                        'type': re.findall('type=\"(.*?)\"', item)[0],\n",
    "                        'category': re.findall('category=\"(.*?)\"', item)[0]\n",
    "                    }\n",
    "                    all_people.append(person)\n",
    "                except:\n",
    "                    pass\n",
    "    return all_people\n",
    "\n",
    "def extract_sentences(doc, tokenizer):\n",
    "    ## You might need to execute\n",
    "    ## nltk.download('punkt') before using nltk\n",
    "    return(tokenizer.tokenize(doc))\n",
    "\n",
    "def find_sent_with_person(p_name, all_sentences):\n",
    "    selected_sentences = []\n",
    "    for doc in all_sentences:\n",
    "        for sentence in doc:\n",
    "            if p_name in sentence:\n",
    "                selected_sentences.append(sentence)\n",
    "    return selected_sentences\n",
    "\n",
    "## files needed for tensorflow projector\n",
    "def save_model(entities_and_embeddings):\n",
    "    with open('word2vec_emb.tsv','w', encoding='utf-8') as vec_file, open('word2vec_meta.tsv','w', encoding='utf-8') as metafile:\n",
    "        for entity, embeddings in entities_and_embeddings.items():\n",
    "            for embedding in embeddings:\n",
    "                vec = [v.numpy() for v in embedding]\n",
    "                vec = np.mean(vec, axis=0)\n",
    "                vec = '\\t'.join(map(str, vec))\n",
    "                vec_file.write(vec+'\\n')\n",
    "                metafile.write(entity+'\\n')\n",
    "\n",
    "def clear_sentence(sentence):\n",
    "    ## Remove digits\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.digits))\n",
    "\n",
    "    ## Remove punctuation marks\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    return sentence\n",
    "\n",
    "def clear_sentence_and_locate_entities(sentence, stopwords=STOPWORDS):\n",
    "    entities_and_used_form = re.findall('<Entity name=\"(.*?)\".*?\">(.*?)</', sentence)\n",
    "\n",
    "    ## Remove entity\n",
    "    ## <Entity name=\"Tomasz Sekielski\" type=\"person\" category=\"dziennikarze\">Tomasz Sekielski</Entity>\n",
    "    ## will result as Tomasz Sekielski\n",
    "    sentence = remove_entity(sentence)\n",
    "\n",
    "    sentence = clear_sentence(sentence)\n",
    "\n",
    "    ## Remove stopwords\n",
    "    cleared_sentence = [word for word in sentence.split() if word not in stopwords and len(word) > 1]\n",
    "    # print(cleared_sentence)\n",
    "    sentence = cleared_sentence\n",
    "    targets = []\n",
    "    already_parsed = 0\n",
    "    for entity, used_form in entities_and_used_form:\n",
    "        used_form = clear_sentence(used_form)\n",
    "        used_form_splitted = used_form.split()\n",
    "        start = sentence[already_parsed:].index(used_form_splitted[0]) + already_parsed\n",
    "        stop = start + len(used_form_splitted)\n",
    "        already_parsed = stop\n",
    "        targets.append({'start': start, 'length': len(used_form_splitted), 'used_form': used_form, 'entity': entity})\n",
    "    # print(targets)\n",
    "\n",
    "    cleared_sentence = [word.lower() for word in cleared_sentence]\n",
    "\n",
    "    return cleared_sentence, targets\n",
    "\n",
    "def get_embedding(sentence, embeddings):\n",
    "    sentence = Sentence(sentence)\n",
    "    embeddings.embed(sentence)\n",
    "    return [token.embedding for token in sentence]\n",
    "\n",
    "def createEmbeddings(name):\n",
    "    if name == 'bert':\n",
    "        return BertEmbeddings('bert-base-multilingual-cased')\n",
    "    if name == 'flair':\n",
    "        return FlairEmbeddings('polish-forward')\n",
    "    if name == 'fasttext':\n",
    "        return FastTextEmbeddings('cc.pl.300.bin')\n",
    "\n",
    "def get_embeddings_of_entity_in_corpus(documents, window_size = 5):\n",
    "    # Polish word embeddings\n",
    "    output = {}\n",
    "    embeddings = createEmbeddings('fasttext')\n",
    "\n",
    "    for document_id, document in enumerate(documents):\n",
    "        for sentence in tqdm(document):\n",
    "            try:\n",
    "                cleared_sentence, targets = clear_sentence_and_locate_entities(sentence)\n",
    "                if len(targets) == 0:\n",
    "                    continue\n",
    "                embeddings_of_tokens = get_embedding(' '.join(cleared_sentence), embeddings)\n",
    "                assert (len(embeddings_of_tokens) == len(cleared_sentence))\n",
    "                for target in targets:\n",
    "                    neighboring_embeddings = [embeddings_of_tokens[target['start'] - window_size: target['start']]] + \\\n",
    "                                             [embeddings_of_tokens[target['start'] + target['length']: target['start'] + target['length'] + window_size]]\n",
    "\n",
    "                    if target['entity'] not in list(output.keys()):\n",
    "                        output[target['entity']] = {document_id: [neighboring_embeddings]}\n",
    "                    elif document_id not in output[target['entity']].keys():\n",
    "                        output[target['entity']][document_id] = [neighboring_embeddings]\n",
    "                    else:\n",
    "                        output[target['entity']][document_id].append(neighboring_embeddings)\n",
    "\n",
    "            except:\n",
    "                print(f\"failed to process: {sentence}\")\n",
    "    return output\n",
    "\n",
    "def save_embeddings(embeddings):\n",
    "    with open('embeddings.pickle', 'wb+') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    docs = load_files(PATH)\n",
    "\n",
    "    ## list all people marked in text\n",
    "    ## returns list of dicts, each person has attr: name, category, type \n",
    "    people = list_people(docs)\n",
    "\n",
    "    ## returns list of sentences in each document\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
    "    documents = [extract_sentences(document, tokenizer) for document in docs]\n",
    "\n",
    "    embeddings = get_embeddings_of_entity_in_corpus(documents, 5)\n",
    "    save_embeddings(embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
